{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating database, uncomment for first run, can be skipped after initial creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python_session"
    }
   },
   "outputs": [],
   "source": [
    "# spark.sql(\n",
    "#     \"CREATE DATABASE IF NOT EXISTS tpch_iceberg LOCATION 's3://<your_bucket>/<path_to_save_files>'\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting default database for scripts, optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python_session"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"use tpch_iceberg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop table. Uncomment if you are recreating the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python_session"
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "drop table customer;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting Iceberg version to 2, compression codec as snappy, and default format as parquet.\n",
    "\n",
    "The partitioning is on c_nationkey.\n",
    "\n",
    "Feel free to play with different formats, partitioning and settings and see how they can affect performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python_session"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS customer (\n",
    "    c_custkey BIGINT,\n",
    "    c_name STRING,\n",
    "    c_address STRING,\n",
    "    c_nationkey BIGINT,\n",
    "    c_phone STRING,\n",
    "    c_acctbal DECIMAL(15,2),\n",
    "    c_mktsegment STRING,\n",
    "    c_comment STRING\n",
    ") USING iceberg\n",
    "PARTITIONED BY (c_nationkey)\n",
    "TBLPROPERTIES (\n",
    "    'write.format.default' = 'parquet',\n",
    "    'write.parquet.compression-codec' = 'snappy',\n",
    "    'format-version' = '2'\n",
    ")\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating schema for the table and loading it from s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python_session"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DecimalType, DateType, LongType\n",
    "\n",
    "# Customer Schema\n",
    "customer_schema = StructType([\n",
    "    StructField(\"c_custkey\", LongType(), False),\n",
    "    StructField(\"c_name\", StringType(), True),\n",
    "    StructField(\"c_address\", StringType(), True),\n",
    "    StructField(\"c_nationkey\", LongType(), True),\n",
    "    StructField(\"c_phone\", StringType(), True),\n",
    "    StructField(\"c_acctbal\", DecimalType(15,2), True),\n",
    "    StructField(\"c_mktsegment\", StringType(), True),\n",
    "    StructField(\"c_comment\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python_session"
    }
   },
   "outputs": [],
   "source": [
    "customer_df = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"delimiter\", \"|\") \\\n",
    "    .schema(customer_schema) \\\n",
    "    .load('s3://redshift-downloads/TPC-H/2.18/3TB/customer/')\n",
    "customer_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the data frame in the customer table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python_session"
    }
   },
   "outputs": [],
   "source": [
    "customer_df.write \\\n",
    "    .format(\"iceberg\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .saveAsTable(\"customer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating the data, should be 450,000,000 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python_session"
    }
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "select count(*) from spark_catalog.tpch_iceberg.customer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create staging table for merge example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncomment for recreating staging table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python_session"
    }
   },
   "outputs": [],
   "source": [
    "# %%sql\n",
    "# drop table customer_stg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create staging table table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python_session"
    }
   },
   "outputs": [],
   "source": [
    "# %%sql\n",
    "# create table customer_stg\n",
    "# USING iceberg\n",
    "# PARTITIONED BY (c_nationkey)\n",
    "# TBLPROPERTIES (\n",
    "#     'write.format.default' = 'parquet',\n",
    "#     'write.parquet.compression-codec' = 'snappy',\n",
    "#     'format-version' = '2')\n",
    "# as select * from customer limit 1000;\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python_session"
    }
   },
   "outputs": [],
   "source": [
    "# %%sql\n",
    "# select * from customer_stg limit 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing data in staging table - to create new keys and update data for existing ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python_session"
    }
   },
   "outputs": [],
   "source": [
    "# %%sql\n",
    "# update customer_stg\n",
    "# set c_custkey = c_custkey * 1000\n",
    "# where c_custkey % 3 = 0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python_session"
    }
   },
   "outputs": [],
   "source": [
    "# %%sql\n",
    "# update customer_stg\n",
    "# set c_nationkey = 18\n",
    "# where c_custkey % 5 = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example for expiring snapshots via Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python_session"
    }
   },
   "outputs": [],
   "source": [
    "# %%sql\n",
    "# CALL spark_catalog.system.expire_snapshots(table => 'tpch_iceberg.customer',older_than => TIMESTAMP '2025-05-05 00:00:00.000', retain_last => 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example for altering table properties setting updates to MoR (should speed up writes, but can potentially slow down reads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python_session"
    }
   },
   "outputs": [],
   "source": [
    "# %%sql\n",
    "# alter table spark_catalog.tpch_iceberg.customer \n",
    "# SET TBLPROPERTIES ('write.update.mode' = 'merge-on-read');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python_session"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Athena PySpark",
   "language": "python",
   "name": "kepler_python_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "Python_Session",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
